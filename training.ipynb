{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_pad_data(file_path, n_time_slices, max_length=999):\n",
    "    # Load data\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    vibration_data = data['data'].flatten()\n",
    "\n",
    "    # Sampling frequency\n",
    "    fs = data.get('fs', 1).flatten()[0]\n",
    "\n",
    "    # Generate spectrogram\n",
    "    # The output Pxx is the segments x freqs array of instantaneous power, freqs is the frequency vector, bins are the centers of the time bins\n",
    "    Pxx, freqs, bins, im = plt.specgram(vibration_data, NFFT=1024, Fs=fs, noverlap=512, scale='dB', mode='magnitude')\n",
    "\n",
    "    # Close the plot as we only need the data\n",
    "    plt.close()\n",
    "\n",
    "    # Select first n time slices\n",
    "    # print(Pxx.shape)\n",
    "    selected_slices = Pxx[:, :n_time_slices]\n",
    "\n",
    "    # Padding\n",
    "    padded_sequence = np.zeros((Pxx.shape[0], max_length))\n",
    "    padded_sequence[:, :spectrogram_sequence.shape[1]] = spectrogram_sequence\n",
    "\n",
    "    return padded_sequence\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Flatten the data\n",
    "    flat_data = data.flatten()\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_data = (flat_data - np.min(flat_data)) / (np.max(flat_data) - np.min(flat_data))\n",
    "\n",
    "    # Reshape it back to the original shape\n",
    "    normalized_data = normalized_data.reshape(data.shape)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# Path to your folder\n",
    "data_folder = r\"C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing dataset\"\n",
    "\n",
    "# List of .mat files\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Number of time slices you want to consider\n",
    "n_time_slices = 50  # Adjust this based on your requirements\n",
    "\n",
    "# Process each file\n",
    "for file in mat_files:\n",
    "    # Load and extract sequence from spectrogram\n",
    "    spectrogram_sequence = load_data(file, n_time_slices)\n",
    "\n",
    "    # Normalize the sequence\n",
    "    normalized_sequence = normalize_data(spectrogram_sequence)\n",
    "    print(normalized_sequence.shape)\n",
    "    break\n",
    "\n",
    "    # Now, normalized_sequence is ready to be used as input to your model\n",
    "    # You can proceed with feeding this into your Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout\n",
    "\n",
    "# Define a simple Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    # Multi-head attention and dropout\n",
    "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # Feed forward and dropout\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "\n",
    "    # Return output\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Define your model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    \n",
    "    # Output layer for prediction\n",
    "    outputs = Dense(1, activation='sigmoid')(x[:, 0, :])\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(num_time_slices=50, d_model=1024, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "num_epochs = 5\n",
    "shuffled_files = mat_files\n",
    "random.shuffle(shuffled_files)\n",
    "training_files = shuffled_files[:math.floor(len(shuffled_files)*0.8)]\n",
    "testing_files = shuffled_files[math.ceil(len(shuffled_files)*0.2):]\n",
    "\n",
    "# Example of training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Load and pad data\n",
    "        padded_sequence = load_and_pad_data(file)\n",
    "        \n",
    "        # Prepare labels and other necessary preprocessing steps\n",
    "        # ...\n",
    "\n",
    "        # Train your model\n",
    "        model.train_on_batch(padded_sequence, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout, MultiHeadAttention\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_and_pad_data(file_path, max_length=999):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    vibration_data = data['data'].flatten()\n",
    "    fs = data.get('fs', 1).flatten()[0]\n",
    "\n",
    "    # Generate spectrogram\n",
    "    Pxx, freqs, bins, im = plt.specgram(vibration_data, NFFT=1024, Fs=fs, noverlap=512, scale='dB', mode='magnitude')\n",
    "    plt.close()\n",
    "\n",
    "    # Normalize spectrogram\n",
    "    Pxx_normalized = (Pxx - np.min(Pxx)) / (np.max(Pxx) - np.min(Pxx))\n",
    "\n",
    "    # Pad spectrogram\n",
    "    padded_sequence = np.zeros((Pxx.shape[0], max_length))\n",
    "    padded_sequence[:, :Pxx_normalized.shape[1]] = Pxx_normalized\n",
    "\n",
    "    return padded_sequence, Pxx_normalized.shape[1]\n",
    "\n",
    "# Label generation and normalization\n",
    "def calculate_and_normalize_labels(sequence_length, max_length, max_time_to_failure):\n",
    "    time_per_slice = 1  # Adjust this based on your data sampling rate\n",
    "    labels = np.array([(max_length - i) * time_per_slice for i in range(sequence_length)])\n",
    "    labels = labels / max_time_to_failure\n",
    "    return np.pad(labels, (0, max_length - sequence_length), 'constant', constant_values=0)\n",
    "\n",
    "# Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Create Transformer model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    outputs = Dense(1)(x[:, 0, :])  # Linear activation for regression\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Main script\n",
    "data_folder = \"your_data_folder_path\"\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Split data into train/test sets\n",
    "random.shuffle(mat_files)\n",
    "split_index = math.floor(len(mat_files) * 0.8)\n",
    "training_files = mat_files[:split_index]\n",
    "testing_files = mat_files[split_index:]\n",
    "\n",
    "# Model parameters\n",
    "num_epochs = 5\n",
    "max_sequence_length = 999\n",
    "max_time_to_failure = 1000  # Adjust based on your dataset\n",
    "num_features = 1024  # Adjust based on the number of frequency bins\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(max_sequence_length, num_features, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Load and pad data\n",
    "        padded_sequence, sequence_length = load_and_pad_data(file, max_sequence_length)\n",
    "\n",
    "        # Calculate and normalize labels\n",
    "        labels = calculate_and_normalize_labels(sequence_length, max_sequence_length, max_time_to_failure)\n",
    "\n",
    "        # Expand dimensions to match input shape for the model\n",
    "        padded_sequence = np.expand_dims(padded_sequence, axis=0)\n",
    "        labels = np.expand_dims(labels, axis=0)\n",
    "\n",
    "        # Train the model\n",
    "        model.train_on_batch(padded_sequence, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout, MultiHeadAttention\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_and_process_data(file_path, sequence_length, start_index):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    spectrogram = data['spectrogram']\n",
    "    \n",
    "    # Select a sequence starting at a random index\n",
    "    if spectrogram.shape[1] > sequence_length:\n",
    "        selected_spectrogram = spectrogram[:, start_index:start_index + sequence_length]\n",
    "    else:\n",
    "        selected_spectrogram = spectrogram\n",
    "\n",
    "    # Normalize spectrogram\n",
    "    Pxx_normalized = (selected_spectrogram - np.min(selected_spectrogram)) / (np.max(selected_spectrogram) - np.min(selected_spectrogram))\n",
    "    return Pxx_normalized\n",
    "\n",
    "# Label generation and normalization\n",
    "def calculate_and_normalize_labels(sequence_length, max_time_to_failure):\n",
    "    time_per_slice = 1  # Adjust this based on your data sampling rate\n",
    "    labels = np.array([(sequence_length - i) * time_per_slice for i in range(sequence_length)])\n",
    "    labels = labels / max_time_to_failure\n",
    "    return labels\n",
    "\n",
    "# Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Create Transformer model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    outputs = Dense(1)(x[:, 0, :])  # Linear activation for regression\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Main script\n",
    "data_folder = r\"C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\IMS\\processed\\sep_spec\"  # Replace with your data folder path\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Split data into train/test sets\n",
    "random.shuffle(mat_files)\n",
    "split_index = math.floor(len(mat_files) * 0.8)\n",
    "training_files = mat_files[:split_index]\n",
    "testing_files = mat_files[split_index:]\n",
    "\n",
    "# Model parameters\n",
    "num_epochs = 5\n",
    "max_sequence_length = 999  # You can adjust this as needed\n",
    "max_time_to_failure = 1000  # Adjust based on your dataset\n",
    "num_features = 1024  # This should match the number of frequency bins in the spectrogram\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(max_sequence_length, num_features, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Determine the sequence length (could be variable) and starting index\n",
    "        sequence_length = random.randint(100, max_sequence_length)  # Example range for variable sequence length\n",
    "        start_index = random.randint(0, max_sequence_length - sequence_length)\n",
    "        \n",
    "        # Load and process data\n",
    "        spectrogram_normalized = load_and_process_data(file, sequence_length, start_index)\n",
    "\n",
    "        # Calculate and normalize labels\n",
    "        labels = calculate_and_normalize_labels(sequence_length, max_time_to_failure)\n",
    "\n",
    "        # Expand dimensions to match input shape for the model\n",
    "        spectrogram_normalized = np.expand_dims(spectrogram_normalized, axis=0)\n",
    "        labels = np.expand_dims(labels, axis=0)\n",
    "\n",
    "        # Train the model\n",
    "        model.train_on_batch(spectrogram_normalized, labels)\n",
    "\n",
    "# Save the model after training\n",
    "model.save(r'C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\models\\first_drafts\\model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import random\n",
    "\n",
    "\n",
    "# Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Create Transformer model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    outputs = Dense(1)(x[:, 0, :])  # Linear activation for regression\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    return data['spectrogram'], data['timestamps'][0]\n",
    "\n",
    "def generate_sequences_and_labels(spectrogram, timestamps, sequence_length, start_index):\n",
    "    end_index = start_index + sequence_length\n",
    "    sequence = spectrogram[:, start_index:end_index]\n",
    "    # Calculate time to failure for the start of the sequence\n",
    "    time_to_failure = timestamps[-1] - timestamps[start_index]\n",
    "    # Normalize sequence\n",
    "    sequence_normalized = (sequence - np.min(sequence)) / (np.max(sequence) - np.min(sequence))\n",
    "    return sequence_normalized, time_to_failure\n",
    "\n",
    "def train_test_split(spectrogram, train_size=0.8):\n",
    "    train_end = int(spectrogram.shape[1] * train_size)\n",
    "    train_data = spectrogram[:, :train_end]\n",
    "    test_data = spectrogram[:, train_end:]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define model parameters\n",
    "num_features = 20480  # This should match the frequency bins in the spectrogram\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "checkpoint_filepath = r'C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\models\\first_drafts/model_checkpoint.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "filepath=checkpoint_filepath,\n",
    "save_weights_only=True,\n",
    "monitor='val_loss',\n",
    "mode='min',\n",
    "save_best_only=True)\n",
    "\n",
    "# Initialize the model with a small sequence length\n",
    "initial_sequence_length = 100  # Start with a small sequence length\n",
    "model = create_transformer_model(initial_sequence_length, num_features, num_heads, dff)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Define paths\n",
    "data_folder = r\"C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\IMS\\processed\\sep_spec\"  # Replace with your data folder path\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "random.shuffle(mat_files)\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 5\n",
    "max_sequence_length = 500  # Maximum sequence length\n",
    "sequence_length_increment = 50  # How much to increment the sequence length\n",
    "increment_interval = 2  # How many epochs to complete before incrementing sequence length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % increment_interval == 0 and epoch > 0:\n",
    "        current_sequence_length = min(initial_sequence_length + sequence_length_increment, max_sequence_length)\n",
    "        # Reinitialize model to accept new sequence length if it changes\n",
    "        model = create_transformer_model(current_sequence_length, num_features, num_heads, dff)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    for file_path in mat_files:\n",
    "        spectrogram, timestamps = load_data(file_path)\n",
    "        train_spectrogram, _ = train_test_split(spectrogram)  # Split data into train and test\n",
    "\n",
    "        # Determine the total duration of the spectrogram in seconds\n",
    "        total_duration = timestamps[-1] - timestamps[0]\n",
    "        \n",
    "        # Create random starting points for sequence generation\n",
    "        num_sequences = (train_spectrogram.shape[1] - current_sequence_length) // batch_size\n",
    "        for _ in range(num_sequences):\n",
    "            # Randomly select a start index for the sequence\n",
    "            start_index = random.randint(0, spectrogram.shape[1] - initial_sequence_length)\n",
    "            sequence, time_to_failure = generate_sequences_and_labels(\n",
    "                spectrogram, timestamps, initial_sequence_length, start_index\n",
    "            )\n",
    "            \n",
    "            # Expand dimensions to match input shape for the model\n",
    "            sequence = np.expand_dims(sequence, axis=0)  # Add batch dimension\n",
    "            \n",
    "            # Normalize time to failure label between 0 and 1\n",
    "            label = time_to_failure / total_duration\n",
    "            label = np.array([[label]])  # Match the expected model output shape\n",
    "            \n",
    "            # Train the model on the sequence\n",
    "            model.train_on_batch(sequence, label)\n",
    "    model.save('models\\idk{epoch}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss = []\n",
    "for file_path in validation_files:\n",
    "    spectrogram, timestamps = load_data(file_path)\n",
    "    validation_spectrogram, _ = train_test_split(spectrogram)\n",
    "\n",
    "for start_index in range(0, validation_spectrogram.shape[1] - initial_sequence_length, batch_size):\n",
    "    sequence, label = generate_sequences_and_labels(\n",
    "        validation_spectrogram, timestamps, initial_sequence_length, start_index)\n",
    "    \n",
    "    # Expand dimensions to match input shape for the model\n",
    "    sequence = np.expand_dims(sequence, axis=0)  # Add batch dimension\n",
    "    label = np.array([[label]])  # Match the expected model output shape\n",
    "    \n",
    "    # Evaluate the model on the validation sequence\n",
    "    val_loss = model.evaluate(sequence, label, verbose=0)\n",
    "    validation_loss.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20480\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "data = scipy.io.loadmat(r'C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\IMS\\processed\\sep_spec\\1_channel_4.mat')\n",
    "spectrogram = data['spectrogram']\n",
    "num_frequency_bins = spectrogram.shape[0]\n",
    "print(num_frequency_bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
