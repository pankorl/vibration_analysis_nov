{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_pad_data(file_path, n_time_slices, max_length=999):\n",
    "    # Load data\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    vibration_data = data['data'].flatten()\n",
    "\n",
    "    # Sampling frequency\n",
    "    fs = data.get('fs', 1).flatten()[0]\n",
    "\n",
    "    # Generate spectrogram\n",
    "    # The output Pxx is the segments x freqs array of instantaneous power, freqs is the frequency vector, bins are the centers of the time bins\n",
    "    Pxx, freqs, bins, im = plt.specgram(vibration_data, NFFT=1024, Fs=fs, noverlap=512, scale='dB', mode='magnitude')\n",
    "\n",
    "    # Close the plot as we only need the data\n",
    "    plt.close()\n",
    "\n",
    "    # Select first n time slices\n",
    "    # print(Pxx.shape)\n",
    "    selected_slices = Pxx[:, :n_time_slices]\n",
    "\n",
    "    # Padding\n",
    "    padded_sequence = np.zeros((Pxx.shape[0], max_length))\n",
    "    padded_sequence[:, :spectrogram_sequence.shape[1]] = spectrogram_sequence\n",
    "\n",
    "    return padded_sequence\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Flatten the data\n",
    "    flat_data = data.flatten()\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_data = (flat_data - np.min(flat_data)) / (np.max(flat_data) - np.min(flat_data))\n",
    "\n",
    "    # Reshape it back to the original shape\n",
    "    normalized_data = normalized_data.reshape(data.shape)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# Path to your folder\n",
    "data_folder = r\"C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing dataset\"\n",
    "\n",
    "# List of .mat files\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Number of time slices you want to consider\n",
    "n_time_slices = 50  # Adjust this based on your requirements\n",
    "\n",
    "# Process each file\n",
    "for file in mat_files:\n",
    "    # Load and extract sequence from spectrogram\n",
    "    spectrogram_sequence = load_data(file, n_time_slices)\n",
    "\n",
    "    # Normalize the sequence\n",
    "    normalized_sequence = normalize_data(spectrogram_sequence)\n",
    "    print(normalized_sequence.shape)\n",
    "    break\n",
    "\n",
    "    # Now, normalized_sequence is ready to be used as input to your model\n",
    "    # You can proceed with feeding this into your Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout\n",
    "\n",
    "# Define a simple Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    # Multi-head attention and dropout\n",
    "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # Feed forward and dropout\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "\n",
    "    # Return output\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Define your model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    \n",
    "    # Output layer for prediction\n",
    "    outputs = Dense(1, activation='sigmoid')(x[:, 0, :])\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(num_time_slices=50, d_model=1024, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "num_epochs = 5\n",
    "shuffled_files = mat_files\n",
    "random.shuffle(shuffled_files)\n",
    "training_files = shuffled_files[:math.floor(len(shuffled_files)*0.8)]\n",
    "testing_files = shuffled_files[math.ceil(len(shuffled_files)*0.2):]\n",
    "\n",
    "# Example of training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Load and pad data\n",
    "        padded_sequence = load_and_pad_data(file)\n",
    "        \n",
    "        # Prepare labels and other necessary preprocessing steps\n",
    "        # ...\n",
    "\n",
    "        # Train your model\n",
    "        model.train_on_batch(padded_sequence, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout, MultiHeadAttention\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_and_pad_data(file_path, max_length=999):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    vibration_data = data['data'].flatten()\n",
    "    fs = data.get('fs', 1).flatten()[0]\n",
    "\n",
    "    # Generate spectrogram\n",
    "    Pxx, freqs, bins, im = plt.specgram(vibration_data, NFFT=1024, Fs=fs, noverlap=512, scale='dB', mode='magnitude')\n",
    "    plt.close()\n",
    "\n",
    "    # Normalize spectrogram\n",
    "    Pxx_normalized = (Pxx - np.min(Pxx)) / (np.max(Pxx) - np.min(Pxx))\n",
    "\n",
    "    # Pad spectrogram\n",
    "    padded_sequence = np.zeros((Pxx.shape[0], max_length))\n",
    "    padded_sequence[:, :Pxx_normalized.shape[1]] = Pxx_normalized\n",
    "\n",
    "    return padded_sequence, Pxx_normalized.shape[1]\n",
    "\n",
    "# Label generation and normalization\n",
    "def calculate_and_normalize_labels(sequence_length, max_length, max_time_to_failure):\n",
    "    time_per_slice = 1  # Adjust this based on your data sampling rate\n",
    "    labels = np.array([(max_length - i) * time_per_slice for i in range(sequence_length)])\n",
    "    labels = labels / max_time_to_failure\n",
    "    return np.pad(labels, (0, max_length - sequence_length), 'constant', constant_values=0)\n",
    "\n",
    "# Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Create Transformer model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    outputs = Dense(1)(x[:, 0, :])  # Linear activation for regression\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Main script\n",
    "data_folder = \"your_data_folder_path\"\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Split data into train/test sets\n",
    "random.shuffle(mat_files)\n",
    "split_index = math.floor(len(mat_files) * 0.8)\n",
    "training_files = mat_files[:split_index]\n",
    "testing_files = mat_files[split_index:]\n",
    "\n",
    "# Model parameters\n",
    "num_epochs = 5\n",
    "max_sequence_length = 999\n",
    "max_time_to_failure = 1000  # Adjust based on your dataset\n",
    "num_features = 1024  # Adjust based on the number of frequency bins\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(max_sequence_length, num_features, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Load and pad data\n",
    "        padded_sequence, sequence_length = load_and_pad_data(file, max_sequence_length)\n",
    "\n",
    "        # Calculate and normalize labels\n",
    "        labels = calculate_and_normalize_labels(sequence_length, max_sequence_length, max_time_to_failure)\n",
    "\n",
    "        # Expand dimensions to match input shape for the model\n",
    "        padded_sequence = np.expand_dims(padded_sequence, axis=0)\n",
    "        labels = np.expand_dims(labels, axis=0)\n",
    "\n",
    "        # Train the model\n",
    "        model.train_on_batch(padded_sequence, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout, MultiHeadAttention\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_and_process_data(file_path, sequence_length, start_index):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    spectrogram = data['spectrogram']\n",
    "    \n",
    "    # Select a sequence starting at a random index\n",
    "    if spectrogram.shape[1] > sequence_length:\n",
    "        selected_spectrogram = spectrogram[:, start_index:start_index + sequence_length]\n",
    "    else:\n",
    "        selected_spectrogram = spectrogram\n",
    "\n",
    "    # Normalize spectrogram\n",
    "    Pxx_normalized = (selected_spectrogram - np.min(selected_spectrogram)) / (np.max(selected_spectrogram) - np.min(selected_spectrogram))\n",
    "    return Pxx_normalized\n",
    "\n",
    "# Label generation and normalization\n",
    "def calculate_and_normalize_labels(sequence_length, max_time_to_failure):\n",
    "    time_per_slice = 1  # Adjust this based on your data sampling rate\n",
    "    labels = np.array([(sequence_length - i) * time_per_slice for i in range(sequence_length)])\n",
    "    labels = labels / max_time_to_failure\n",
    "    return labels\n",
    "\n",
    "# Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Create Transformer model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    outputs = Dense(1)(x[:, 0, :])  # Linear activation for regression\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Main script\n",
    "data_folder = r\"C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\IMS\\processed\\sep_spec\"  # Replace with your data folder path\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Split data into train/test sets\n",
    "random.shuffle(mat_files)\n",
    "split_index = math.floor(len(mat_files) * 0.8)\n",
    "training_files = mat_files[:split_index]\n",
    "testing_files = mat_files[split_index:]\n",
    "\n",
    "# Model parameters\n",
    "num_epochs = 5\n",
    "max_sequence_length = 999  # You can adjust this as needed\n",
    "max_time_to_failure = 1000  # Adjust based on your dataset\n",
    "num_features = 1024  # This should match the number of frequency bins in the spectrogram\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(max_sequence_length, num_features, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Determine the sequence length (could be variable) and starting index\n",
    "        sequence_length = random.randint(100, max_sequence_length)  # Example range for variable sequence length\n",
    "        start_index = random.randint(0, max_sequence_length - sequence_length)\n",
    "        \n",
    "        # Load and process data\n",
    "        spectrogram_normalized = load_and_process_data(file, sequence_length, start_index)\n",
    "\n",
    "        # Calculate and normalize labels\n",
    "        labels = calculate_and_normalize_labels(sequence_length, max_time_to_failure)\n",
    "\n",
    "        # Expand dimensions to match input shape for the model\n",
    "        spectrogram_normalized = np.expand_dims(spectrogram_normalized, axis=0)\n",
    "        labels = np.expand_dims(labels, axis=0)\n",
    "\n",
    "        # Train the model\n",
    "        model.train_on_batch(spectrogram_normalized, labels)\n",
    "\n",
    "# Save the model after training\n",
    "model.save(r'C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\models\\first_drafts\\model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
