{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 999)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_pad_data(file_path, n_time_slices, max_length=999):\n",
    "    # Load data\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    vibration_data = data['data'].flatten()\n",
    "\n",
    "    # Sampling frequency\n",
    "    fs = data.get('fs', 1).flatten()[0]\n",
    "\n",
    "    # Generate spectrogram\n",
    "    # The output Pxx is the segments x freqs array of instantaneous power, freqs is the frequency vector, bins are the centers of the time bins\n",
    "    Pxx, freqs, bins, im = plt.specgram(vibration_data, NFFT=1024, Fs=fs, noverlap=512, scale='dB', mode='magnitude')\n",
    "\n",
    "    # Close the plot as we only need the data\n",
    "    plt.close()\n",
    "\n",
    "    # Select first n time slices\n",
    "    # print(Pxx.shape)\n",
    "    selected_slices = Pxx[:, :n_time_slices]\n",
    "\n",
    "    # Padding\n",
    "    padded_sequence = np.zeros((Pxx.shape[0], max_length))\n",
    "    padded_sequence[:, :spectrogram_sequence.shape[1]] = spectrogram_sequence\n",
    "\n",
    "    return padded_sequence\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Flatten the data\n",
    "    flat_data = data.flatten()\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_data = (flat_data - np.min(flat_data)) / (np.max(flat_data) - np.min(flat_data))\n",
    "\n",
    "    # Reshape it back to the original shape\n",
    "    normalized_data = normalized_data.reshape(data.shape)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# Path to your folder\n",
    "data_folder = r\"C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing dataset\"\n",
    "\n",
    "# List of .mat files\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Number of time slices you want to consider\n",
    "n_time_slices = 50  # Adjust this based on your requirements\n",
    "\n",
    "# Process each file\n",
    "for file in mat_files:\n",
    "    # Load and extract sequence from spectrogram\n",
    "    spectrogram_sequence = load_data(file, n_time_slices)\n",
    "\n",
    "    # Normalize the sequence\n",
    "    normalized_sequence = normalize_data(spectrogram_sequence)\n",
    "    print(normalized_sequence.shape)\n",
    "    break\n",
    "\n",
    "    # Now, normalized_sequence is ready to be used as input to your model\n",
    "    # You can proceed with feeding this into your Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout\n",
    "\n",
    "# Define a simple Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    # Multi-head attention and dropout\n",
    "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # Feed forward and dropout\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "\n",
    "    # Return output\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Define your model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    \n",
    "    # Output layer for prediction\n",
    "    outputs = Dense(1, activation='sigmoid')(x[:, 0, :])\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(num_time_slices=50, d_model=1024, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "num_epochs = 5\n",
    "shuffled_files = mat_files\n",
    "random.shuffle(shuffled_files)\n",
    "training_files = shuffled_files[:math.floor(len(shuffled_files)*0.8)]\n",
    "testing_files = shuffled_files[math.ceil(len(shuffled_files)*0.2):]\n",
    "\n",
    "# Example of training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Load and pad data\n",
    "        padded_sequence = load_and_pad_data(file)\n",
    "        \n",
    "        # Prepare labels and other necessary preprocessing steps\n",
    "        # ...\n",
    "\n",
    "        # Train your model\n",
    "        model.train_on_batch(padded_sequence, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.15.0\n",
      "  Downloading tensorflow_intel-2.15.0-cp39-cp39-win_amd64.whl (300.8 MB)\n",
      "     -------------------------------------- 300.8/300.8 MB 6.7 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 44.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "     ------------------------------------- 442.0/442.0 kB 27.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\simon\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.4 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.2-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     ---------------------------------------- 413.4/413.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.0-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 33.7 MB/s eta 0:00:00\n",
      "Collecting numpy<2.0.0,>=1.23.5\n",
      "  Downloading numpy-1.26.3-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "     --------------------------------------- 15.8/15.8 MB 28.5 MB/s eta 0:00:00\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 36.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (69.0.2)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 19.2 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl (938 kB)\n",
      "     ------------------------------------- 938.4/938.4 kB 58.0 MB/s eta 0:00:00\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.4.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     --------------------------------------- 24.4/24.4 MB 25.2 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "     -------------------------------------- 130.2/130.2 kB 7.5 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.26.2-py2.py3-none-any.whl (186 kB)\n",
      "     ------------------------------------- 186.5/186.5 kB 11.0 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "     ---------------------------------------- 103.9/103.9 kB ? eta 0:00:00\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.4-cp39-cp39-win_amd64.whl (422 kB)\n",
      "     ------------------------------------- 422.5/422.5 kB 25.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->tensorflow-intel==2.15.0->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.3/181.3 kB 11.4 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\simon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.8.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.9/84.9 kB ? eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, wheel, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, opt-einsum, ml-dtypes, markdown, h5py, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.3\n",
      "    Uninstalling numpy-1.23.3:\n",
      "      Successfully uninstalled numpy-1.23.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\simon\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout, MultiHeadAttention\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_and_pad_data(file_path, max_length=999):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    vibration_data = data['data'].flatten()\n",
    "    fs = data.get('fs', 1).flatten()[0]\n",
    "\n",
    "    # Generate spectrogram\n",
    "    Pxx, freqs, bins, im = plt.specgram(vibration_data, NFFT=1024, Fs=fs, noverlap=512, scale='dB', mode='magnitude')\n",
    "    plt.close()\n",
    "\n",
    "    # Normalize spectrogram\n",
    "    Pxx_normalized = (Pxx - np.min(Pxx)) / (np.max(Pxx) - np.min(Pxx))\n",
    "\n",
    "    # Pad spectrogram\n",
    "    padded_sequence = np.zeros((Pxx.shape[0], max_length))\n",
    "    padded_sequence[:, :Pxx_normalized.shape[1]] = Pxx_normalized\n",
    "\n",
    "    return padded_sequence, Pxx_normalized.shape[1]\n",
    "\n",
    "# Label generation and normalization\n",
    "def calculate_and_normalize_labels(sequence_length, max_length, max_time_to_failure):\n",
    "    time_per_slice = 1  # Adjust this based on your data sampling rate\n",
    "    labels = np.array([(max_length - i) * time_per_slice for i in range(sequence_length)])\n",
    "    labels = labels / max_time_to_failure\n",
    "    return np.pad(labels, (0, max_length - sequence_length), 'constant', constant_values=0)\n",
    "\n",
    "# Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Create Transformer model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    outputs = Dense(1)(x[:, 0, :])  # Linear activation for regression\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Main script\n",
    "data_folder = \"your_data_folder_path\"\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Split data into train/test sets\n",
    "random.shuffle(mat_files)\n",
    "split_index = math.floor(len(mat_files) * 0.8)\n",
    "training_files = mat_files[:split_index]\n",
    "testing_files = mat_files[split_index:]\n",
    "\n",
    "# Model parameters\n",
    "num_epochs = 5\n",
    "max_sequence_length = 999\n",
    "max_time_to_failure = 1000  # Adjust based on your dataset\n",
    "num_features = 1024  # Adjust based on the number of frequency bins\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(max_sequence_length, num_features, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Load and pad data\n",
    "        padded_sequence, sequence_length = load_and_pad_data(file, max_sequence_length)\n",
    "\n",
    "        # Calculate and normalize labels\n",
    "        labels = calculate_and_normalize_labels(sequence_length, max_sequence_length, max_time_to_failure)\n",
    "\n",
    "        # Expand dimensions to match input shape for the model\n",
    "        padded_sequence = np.expand_dims(padded_sequence, axis=0)\n",
    "        labels = np.expand_dims(labels, axis=0)\n",
    "\n",
    "        # Train the model\n",
    "        model.train_on_batch(padded_sequence, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
