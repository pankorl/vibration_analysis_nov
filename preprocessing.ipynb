{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 999)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_pad_data(file_path, n_time_slices, max_length=999):\n",
    "    # Load data\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    vibration_data = data['data'].flatten()\n",
    "\n",
    "    # Sampling frequency\n",
    "    fs = data.get('fs', 1).flatten()[0]\n",
    "\n",
    "    # Generate spectrogram\n",
    "    # The output Pxx is the segments x freqs array of instantaneous power, freqs is the frequency vector, bins are the centers of the time bins\n",
    "    Pxx, freqs, bins, im = plt.specgram(vibration_data, NFFT=1024, Fs=fs, noverlap=512, scale='dB', mode='magnitude')\n",
    "\n",
    "    # Close the plot as we only need the data\n",
    "    plt.close()\n",
    "\n",
    "    # Select first n time slices\n",
    "    # print(Pxx.shape)\n",
    "    selected_slices = Pxx[:, :n_time_slices]\n",
    "\n",
    "    # Padding\n",
    "    padded_sequence = np.zeros((Pxx.shape[0], max_length))\n",
    "    padded_sequence[:, :spectrogram_sequence.shape[1]] = spectrogram_sequence\n",
    "\n",
    "    return padded_sequence\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Flatten the data\n",
    "    flat_data = data.flatten()\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_data = (flat_data - np.min(flat_data)) / (np.max(flat_data) - np.min(flat_data))\n",
    "\n",
    "    # Reshape it back to the original shape\n",
    "    normalized_data = normalized_data.reshape(data.shape)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# Path to your folder\n",
    "data_folder = r\"C:\\Users\\simon\\signal_analysis\\vibration_anal\\vibration_analysis_nov\\data\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing a practical dataset for ball bearing fault diagnosis\\HUST bearing dataset\"\n",
    "\n",
    "# List of .mat files\n",
    "mat_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "\n",
    "# Number of time slices you want to consider\n",
    "n_time_slices = 50  # Adjust this based on your requirements\n",
    "\n",
    "# Process each file\n",
    "for file in mat_files:\n",
    "    # Load and extract sequence from spectrogram\n",
    "    spectrogram_sequence = load_data(file, n_time_slices)\n",
    "\n",
    "    # Normalize the sequence\n",
    "    normalized_sequence = normalize_data(spectrogram_sequence)\n",
    "    print(normalized_sequence.shape)\n",
    "    break\n",
    "\n",
    "    # Now, normalized_sequence is ready to be used as input to your model\n",
    "    # You can proceed with feeding this into your Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, Dropout\n",
    "\n",
    "# Define a simple Transformer block\n",
    "def transformer_block(inputs, num_heads, dff, rate=0.1):\n",
    "    # Multi-head attention and dropout\n",
    "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dff)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # Feed forward and dropout\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "\n",
    "    # Return output\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Define your model\n",
    "def create_transformer_model(num_time_slices, d_model, num_heads, dff):\n",
    "    inputs = Input(shape=(num_time_slices, d_model))\n",
    "    x = transformer_block(inputs, num_heads, dff)\n",
    "    \n",
    "    # Output layer for prediction\n",
    "    outputs = Dense(1, activation='sigmoid')(x[:, 0, :])\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Initialize the model\n",
    "model = create_transformer_model(num_time_slices=50, d_model=1024, num_heads=8, dff=2048)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "num_epochs = 5\n",
    "shuffled_files = mat_files\n",
    "random.shuffle(shuffled_files)\n",
    "training_files = shuffled_files[:math.floor(len(shuffled_files)*0.8)]\n",
    "testing_files = shuffled_files[math.ceil(len(shuffled_files)*0.2):]\n",
    "\n",
    "# Example of training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for file in training_files:\n",
    "        # Load and pad data\n",
    "        padded_sequence = load_and_pad_data(file)\n",
    "        \n",
    "        # Prepare labels and other necessary preprocessing steps\n",
    "        # ...\n",
    "\n",
    "        # Train your model\n",
    "        model.train_on_batch(padded_sequence, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
